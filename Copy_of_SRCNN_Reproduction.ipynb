{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of SRCNN_Reproduction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sieg-Arash/Machine-Learning/blob/master/Copy_of_SRCNN_Reproduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMZ0OAs6V7Vu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SRCNN Test in Google Colab\n",
        "# Upload zip file\n",
        "src = list(files.upload().values())[0]\n",
        "#!unzip yang91.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWRoHXEumsGw",
        "colab_type": "code",
        "outputId": "acf54800-6248-4e25-ed0f-3dc832262878",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "!pip install pillow\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGD50gh5lTRq",
        "colab_type": "code",
        "outputId": "66f5267b-90fc-4887-c135-330aaf8ffe0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import files\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Conv2D, Activation, Input\n",
        "from keras import optimizers\n",
        "from keras.models import load_model\n",
        "from keras.utils import plot_model\n",
        "import numpy as np\n",
        "import scipy.misc\n",
        "import scipy.ndimage\n",
        "import cv2\n",
        "import math\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBDiFqEFBKe8",
        "colab_type": "code",
        "outputId": "03e215d6-b645-4929-db9e-508aa353f2cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9DSLlvhErqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_model(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkQu65Cej3Bv",
        "colab_type": "code",
        "outputId": "4a598498-9a08-4f04-de3b-934998082413",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history =model.fit(X_train, Y_train, batch_size = 16, epochs = 3, validation_data=(X_val, Y_val),verbose=1)\n",
        "\n",
        "# # 绘制训练 & 验证的准确率值\n",
        "# plt.plot(history.history['acc'])\n",
        "# plt.plot(history.history['val_acc'])\n",
        "# plt.title('Model accuracy')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.legend(['Train', 'Test'], loc='upper left')\n",
        "# plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 15959 samples, validate on 1243 samples\n",
            "Epoch 1/3\n",
            "15959/15959 [==============================] - 111s 7ms/step - loss: 473.6877 - val_loss: 335.3044\n",
            "Epoch 2/3\n",
            "15959/15959 [==============================] - 110s 7ms/step - loss: 353.8565 - val_loss: 292.2726\n",
            "Epoch 3/3\n",
            "15959/15959 [==============================] - 109s 7ms/step - loss: 323.1272 - val_loss: 272.8585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGuUcyURkr5M",
        "colab_type": "code",
        "outputId": "52eae001-15bd-4f82-c0df-cb35558f30ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# 绘制训练 & 验证的损失值\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ezw_9NW-oCGx",
        "colab_type": "code",
        "outputId": "de75f478-cb97-4854-82e4-14dc513357b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "scipy.misc.toimage()\n",
        "plt.imshow()\n",
        "%%writefile model.py\n",
        "!cat main.py\n",
        "FLAGS.remove_flag_values(FLAGS.flag_values_dict())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: `toimage` is deprecated!\n",
            "`toimage` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use Pillow's ``Image.fromarray`` directly instead.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACcklEQVR4nAXBPY5dZRAE0KrqvvfN\njwkICECk7IKtsCtERkBGCjEBEshyYokEgZBMhIWxsTV4YPze/bq7OIef9XFe41rjy/Hw+tMv3r8/\nY0EcBxGddWlPeS3WGed3H9xAbpuTFqeRVRc35mwte+aKSPUAUxQHmRePvTyrxj03H6qqDYxaBo1s\nYJY91ar2o090PhoaiwUQSC83fMClqr79KNyAxqaKNMXoY12OAobjfQcYsFzLQjeEbld5RvC+8NA4\nDoiJWaaBvBzjKmAm69wPsZ1RvUGSrY2hhrpqwHAmcNr3XZoa71IpnDNj0gu5YifPiGuvcSNck3RW\n11Eu5hJ770MRG2uMBjCg6qGVgDsViPt/hyBGwRlpWukAXJXuzmD9dtCDIDjg1EDu49IAgdPKwA9v\nzJBtYKCuiwYzF14TxmbePP0JmEsDIKtzCw0MuKfh4ab7xxVK2HBXI0IiHJoJHGac+PjlKU8bjMg9\n2dZgD4AKmAROPz+Zqz2DbChtapx7V7LIYOz7P9++1AJyF6aBJc5xSMtgeoKpJ88kIaSgB1DNUaTN\n6lkd2+2bZ9qTWLknAxAmwqNtNma0M9bz2QhgAEakGmZEMjasOHFy//FPxYn2gY3rnBqI1ZENTqTp\nX379GDTAOny56JAIeAwaXdivX34/j/Iq63z/5tXfL6SuVbEFMCbtzPjur+sboB7u7149f6HEjDC1\nIoJoe7v9/evLLs757d3d2/9EABkz2WQ1Q3HqL5/mFeX17tIlR+6SBIZEAr45vfjqbjtt0TWUeABV\nlKsmMwapG3zzvWMjPGM5UVPt0DQCQq68ff35H1en9272cfwPi73ajWyZzYkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=32x32 at 0x7F4763CAD0B8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkOwNOX1o7KN",
        "colab_type": "code",
        "outputId": "3ba77a32-9ef2-4016-e432-06a578658c00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "FLAGS.remove_flag_values(FLAGS.flag_values_dict())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b090619603d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_flag_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflag_values_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'FLAGS' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsn1LffxtxCJ",
        "colab_type": "code",
        "outputId": "6e6bb7e1-e333-4b42-bb5d-8d2a3a66dbf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "!git clone https://github.com/yifanw90/FSRCNN-TensorFlow\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'FSRCNN-TensorFlow'...\n",
            "remote: Enumerating objects: 176, done.\u001b[K\n",
            "remote: Total 176 (delta 0), reused 0 (delta 0), pack-reused 176\u001b[K\n",
            "Receiving objects: 100% (176/176), 19.26 MiB | 30.43 MiB/s, done.\n",
            "Resolving deltas: 100% (37/37), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bfsrefq6o8le",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 expand_data0.py Train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFANlvFbvsHu",
        "colab_type": "code",
        "outputId": "faa58930-aba4-434d-e3f4-f26c19aeeb80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd FSRCNN-TensorFlow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/FSRCNN-TensorFlow\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O88PalJPwCHV",
        "colab_type": "code",
        "outputId": "9c597f64-f219-4238-e345-c118250ecd8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%%writefile expand_data.py\n",
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pdb\n",
        "\n",
        "# Artifically expands the dataset by a factor of 19 by scaling and then rotating every image\n",
        "def main():\n",
        "  if len(sys.argv) == 2:\n",
        "    data = prepare_data(sys.argv[1])\n",
        "  else:\n",
        "    print(\"Missing argument: You must specify a folder with images to expand\")\n",
        "    return\n",
        "\n",
        "  for i in range(len(data)):\n",
        "    scale(data[i])\n",
        "    rotate(data[i])\n",
        "\n",
        "def prepare_data(dataset):\n",
        "  filenames = os.listdir(dataset)\n",
        "  data_dir = os.path.join(os.getcwd(), dataset)\n",
        "  data = glob.glob(os.path.join(data_dir, \"*.bmp\"))\n",
        "\n",
        "  return data\n",
        "\n",
        "def scale(file):\n",
        "  image = Image.open(file)\n",
        "  width, height = image.size\n",
        "\n",
        "  scales = [0.9, 0.8, 0.7, 0.6]\n",
        "  for scale in scales:\n",
        "    new_width, new_height = int(width * scale), int(height * scale)\n",
        "    new_image = image.resize((new_width, new_height), Image.ANTIALIAS)\n",
        "    new_path = '{}-{}.bmp'.format(file[:-4], scale)\n",
        "    new_image.save(new_path)\n",
        "\n",
        "def rotate(file):\n",
        "  image = Image.open(file)\n",
        "\n",
        "  rotations = [90, 180, 270]\n",
        "  for rotation in rotations:\n",
        "    new_image = image.rotate(rotation, expand=True)\n",
        "    new_path = '{}-{}.bmp'.format(file[:-4], rotation)\n",
        "    new_image.save(new_path)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting expand_data.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9YTbFph1fhm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "warnings.filterwarnings('error')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzzTEJlXxUxo",
        "colab_type": "code",
        "outputId": "59cca374-3a02-442b-b792-efcba066815f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        }
      },
      "source": [
        "!python3  main.py "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'batch_size': <absl.flags._flag.Flag object at 0x7f7ccb4302b0>,\n",
            " 'c_dim': <absl.flags._flag.Flag object at 0x7f7ca2f8ebe0>,\n",
            " 'checkpoint_dir': <absl.flags._flag.Flag object at 0x7f7ca2f8edd8>,\n",
            " 'data_dir': <absl.flags._flag.Flag object at 0x7f7ca2f8eeb8>,\n",
            " 'epoch': <absl.flags._flag.Flag object at 0x7f7ccb4297b8>,\n",
            " 'fast': <absl.flags._flag.BooleanFlag object at 0x7f7ccb403668>,\n",
            " 'h': <tensorflow.python.platform.app._HelpFlag object at 0x7f7ca2fa2128>,\n",
            " 'help': <tensorflow.python.platform.app._HelpFlag object at 0x7f7ca2fa2128>,\n",
            " 'helpfull': <tensorflow.python.platform.app._HelpfullFlag object at 0x7f7ca2fa2198>,\n",
            " 'helpshort': <tensorflow.python.platform.app._HelpshortFlag object at 0x7f7ca2fa2208>,\n",
            " 'learning_rate': <absl.flags._flag.Flag object at 0x7f7ca2f80208>,\n",
            " 'momentum': <absl.flags._flag.Flag object at 0x7f7ca2f802b0>,\n",
            " 'output_dir': <absl.flags._flag.Flag object at 0x7f7ca2f8ee48>,\n",
            " 'params': <absl.flags._flag.BooleanFlag object at 0x7f7ca2fa2048>,\n",
            " 'scale': <absl.flags._flag.Flag object at 0x7f7ca2f8ec88>,\n",
            " 'stride': <absl.flags._flag.Flag object at 0x7f7ca2f8ed30>,\n",
            " 'threads': <absl.flags._flag.Flag object at 0x7f7ca2f8efd0>,\n",
            " 'train': <absl.flags._flag.BooleanFlag object at 0x7f7ca2f8ef28>}\n",
            "2019-05-23 09:03:50.227228: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-23 09:03:50.227457: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2af69a0 executing computations on platform Host. Devices:\n",
            "2019-05-23 09:03:50.227491: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            " [*] Reading checkpoints...\n",
            " [!] Load failed...\n",
            "Beginning training setup...\n",
            "<class 'int'> <class 'int'> <class 'int'>\n",
            "Training setup took 0.8838975429534912 seconds with 1 threads\n",
            "Total setup time took 0.8876104354858398 seconds with 1 threads\n",
            "Training...\n",
            "Epoch: [ 2], step: [10], time: [0.8437], loss: [4.15879107]\n",
            "Epoch: [ 4], step: [20], time: [1.5094], loss: [5.02475309]\n",
            "Epoch: [ 5], step: [30], time: [2.1796], loss: [2.61630821]\n",
            "Epoch: [ 7], step: [40], time: [2.8497], loss: [2.63520789]\n",
            "Epoch: [ 9], step: [50], time: [3.5227], loss: [1.22016263]\n",
            "checkpoint/fsrcnn_21/FSRCNN.model\n",
            "Epoch: [10], step: [60], time: [4.2812], loss: [1.10474396]\n",
            "Start Average: [7.837587], End Average: [1.337930], Improved: [82.93%]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGO9uQKLr0U4",
        "colab_type": "code",
        "outputId": "3d163b1d-fc8e-498c-81a6-e4d63a671c4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%%writefile model.py\n",
        "from utils import (\n",
        "  read_data, \n",
        "  thread_train_setup,\n",
        "  train_input_setup,\n",
        "  test_input_setup,\n",
        "  save_params,\n",
        "  merge,\n",
        "  array_image_save\n",
        ")\n",
        "\n",
        "import time\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from PIL import Image\n",
        "import pdb\n",
        "\n",
        "# Based on http://mmlab.ie.cuhk.edu.hk/projects/FSRCNN.html\n",
        "class FSRCNN(object):\n",
        "  \n",
        "  def __init__(self, sess, config):\n",
        "    self.sess = sess\n",
        "    self.fast = config.fast\n",
        "    self.train = config.train\n",
        "    self.c_dim = config.c_dim\n",
        "    self.is_grayscale = (self.c_dim == 1)\n",
        "    self.epoch = config.epoch\n",
        "    self.scale = config.scale\n",
        "    self.stride = config.stride\n",
        "    self.batch_size = config.batch_size\n",
        "    self.learning_rate = config.learning_rate\n",
        "    self.momentum = config.momentum\n",
        "    self.threads = config.threads\n",
        "    self.params = config.params\n",
        "\n",
        "    # Different image/label sub-sizes for different scaling factors x2, x3, x4\n",
        "    scale_factors = [[14, 20], [11, 21], [10, 24]]\n",
        "    self.image_size, self.label_size = scale_factors[self.scale - 2]\n",
        "    # Testing uses different strides to ensure sub-images line up correctly\n",
        "    if not self.train:\n",
        "      self.stride = [10, 7, 6][self.scale - 2]\n",
        "\n",
        "    # Different model layer counts and filter sizes for FSRCNN vs FSRCNN-s (fast), (s, d, m) in paper\n",
        "    model_params = [[56, 12, 4], [32, 5, 1]]\n",
        "    self.model_params = model_params[self.fast]\n",
        "    \n",
        "    self.checkpoint_dir = config.checkpoint_dir\n",
        "    self.output_dir = config.output_dir\n",
        "    self.data_dir = config.data_dir\n",
        "    self.build_model()\n",
        "\n",
        "\n",
        "  def build_model(self):\n",
        "    self.images = tf.placeholder(tf.float32, [None, self.image_size, self.image_size, self.c_dim], name='images')\n",
        "    self.labels = tf.placeholder(tf.float32, [None, self.label_size, self.label_size, self.c_dim], name='labels')\n",
        "    # Batch size differs in training vs testing\n",
        "    self.batch = tf.placeholder(tf.int32, shape=[], name='batch')\n",
        "\n",
        "    # FSCRNN-s (fast) has smaller filters and less layers but can achieve faster performance\n",
        "    s, d, m = self.model_params\n",
        "\n",
        "    expand_weight, deconv_weight = 'w{}'.format(m + 3), 'w{}'.format(m + 4)\n",
        "    self.weights = {\n",
        "      'w1': tf.Variable(tf.random_normal([5, 5, 1, s], stddev=0.0378, dtype=tf.float32), name='w1'),\n",
        "      'w2': tf.Variable(tf.random_normal([1, 1, s, d], stddev=0.3536, dtype=tf.float32), name='w2'),\n",
        "      expand_weight: tf.Variable(tf.random_normal([1, 1, d, s], stddev=0.189, dtype=tf.float32), name=expand_weight),\n",
        "      deconv_weight: tf.Variable(tf.random_normal([9, 9, 1, s], stddev=0.0001, dtype=tf.float32), name=deconv_weight)\n",
        "    }\n",
        "\n",
        "    expand_bias, deconv_bias = 'b{}'.format(m + 3), 'b{}'.format(m + 4)\n",
        "    self.biases = {\n",
        "      'b1': tf.Variable(tf.zeros([s]), name='b1'),\n",
        "      'b2': tf.Variable(tf.zeros([d]), name='b2'),\n",
        "      expand_bias: tf.Variable(tf.zeros([s]), name=expand_bias),\n",
        "      deconv_bias: tf.Variable(tf.zeros([1]), name=deconv_bias)\n",
        "    }\n",
        "\n",
        "    # Create the m mapping layers weights/biases\n",
        "    for i in range(3, m + 3):\n",
        "      weight_name, bias_name = 'w{}'.format(i), 'b{}'.format(i)\n",
        "      self.weights[weight_name] = tf.Variable(tf.random_normal([3, 3, d, d], stddev=0.1179, dtype=tf.float32), name=weight_name)\n",
        "      self.biases[bias_name] = tf.Variable(tf.zeros([d]), name=bias_name)\n",
        "\n",
        "    self.pred = self.model()\n",
        "\n",
        "    # Loss function (MSE)\n",
        "    self.loss = tf.reduce_mean(tf.reduce_sum(tf.square(self.labels - self.pred), reduction_indices=0))\n",
        "    \n",
        "    self.saver = tf.train.Saver()\n",
        "\n",
        "  def run(self):\n",
        "    # SGD with momentum\n",
        "    self.train_op = tf.train.MomentumOptimizer(self.learning_rate, self.momentum).minimize(self.loss)\n",
        "\n",
        "    tf.global_variables_initializer().run()\n",
        "\n",
        "    if self.load(self.checkpoint_dir):\n",
        "      print(\" [*] Load SUCCESS\")\n",
        "    else:\n",
        "      print(\" [!] Load failed...\")\n",
        "\n",
        "    if self.params:\n",
        "      save_params(self.sess, self.weights, self.biases)\n",
        "    elif self.train:\n",
        "      self.run_train()\n",
        "    else:\n",
        "      self.run_test()\n",
        "\n",
        "  def run_train(self):\n",
        "    start_time = time.time()\n",
        "    print(\"Beginning training setup...\")\n",
        "    if self.threads == 1:\n",
        "      train_input_setup(self)\n",
        "    else:\n",
        "      thread_train_setup(self)\n",
        "    print(\"Training setup took {} seconds with {} threads\".format(time.time() - start_time, self.threads))\n",
        "\n",
        "    data_dir = os.path.join('./{}'.format(self.checkpoint_dir), \"train.h5\")\n",
        "    train_data, train_label = read_data(data_dir)\n",
        "    print(\"Total setup time took {} seconds with {} threads\".format(time.time() - start_time, self.threads))\n",
        "\n",
        "    print(\"Training...\")\n",
        "    start_time = time.time()\n",
        "    start_average, end_average, counter = 0, 0, 0\n",
        "\n",
        "    for ep in range(self.epoch):\n",
        "      # Run by batch images\n",
        "      batch_idxs = len(train_data) // self.batch_size\n",
        "      batch_average = 0\n",
        "      for idx in range(0, batch_idxs):\n",
        "        batch_images = train_data[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "        batch_labels = train_label[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "\n",
        "        counter += 1\n",
        "        _, err = self.sess.run([self.train_op, self.loss], feed_dict={self.images: batch_images, self.labels: batch_labels, self.batch: self.batch_size})\n",
        "        batch_average += err\n",
        "\n",
        "        if counter % 10 == 0:\n",
        "          print(\"Epoch: [%2d], step: [%2d], time: [%4.4f], loss: [%.8f]\" \\\n",
        "            % ((ep+1), counter, time.time() - start_time, err))\n",
        "\n",
        "        # Save every 500 steps\n",
        "        if counter % 50 == 0:\n",
        "          self.save(self.checkpoint_dir, counter)\n",
        "\n",
        "      batch_average = float(batch_average) / batch_idxs\n",
        "      if ep < (self.epoch * 0.2):\n",
        "        start_average += batch_average\n",
        "      elif ep >= (self.epoch * 0.8):\n",
        "        end_average += batch_average\n",
        "\n",
        "    # Compare loss of the first 20% and the last 20% epochs\n",
        "    start_average = float(start_average) / (self.epoch * 0.2)\n",
        "    end_average = float(end_average) / (self.epoch * 0.2)\n",
        "    print(\"Start Average: [%.6f], End Average: [%.6f], Improved: [%.2f%%]\" \\\n",
        "      % (start_average, end_average, 100 - (100*end_average/start_average)))\n",
        "\n",
        "    # Linux desktop notification when training has been completed\n",
        "    # title = \"Training complete - FSRCNN\"\n",
        "    # notification = \"{}-{}-{} done training after {} epochs\".format(self.image_size, self.label_size, self.stride, self.epoch);\n",
        "    # notify_command = 'notify-send \"{}\" \"{}\"'.format(title, notification)\n",
        "    # os.system(notify_command)\n",
        "\n",
        "  \n",
        "  def run_test(self):\n",
        "    nx, ny = test_input_setup(self)\n",
        "    data_dir = os.path.join('./{}'.format(self.checkpoint_dir), \"test.h5\")\n",
        "    test_data, test_label = read_data(data_dir)\n",
        "\n",
        "    print(\"Testing...\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    result = self.pred.eval({self.images: test_data, self.labels: test_label, self.batch: nx * ny})\n",
        "    print(\"Took %.3f seconds\" % (time.time() - start_time))\n",
        "\n",
        "    result = merge(result, [nx, ny])\n",
        "    result = result.squeeze()\n",
        "    image_path = os.path.join(os.getcwd(), self.output_dir)\n",
        "    image_path = os.path.join(image_path, \"test_image.png\")\n",
        "\n",
        "    array_image_save(result * 255, image_path)\n",
        "\n",
        "  def model(self):\n",
        "    # Feature Extraction\n",
        "    conv_feature = self.prelu(tf.nn.conv2d(self.images, self.weights['w1'], strides=[1,1,1,1], padding='VALID') + self.biases['b1'], 1)\n",
        "\n",
        "    # Shrinking\n",
        "    conv_shrink = self.prelu(tf.nn.conv2d(conv_feature, self.weights['w2'], strides=[1,1,1,1], padding='SAME') + self.biases['b2'], 2)\n",
        "\n",
        "    # Mapping (# mapping layers = m)\n",
        "    prev_layer, m = conv_shrink, self.model_params[2]\n",
        "    for i in range(3, m + 3):\n",
        "      weights, biases = self.weights['w{}'.format(i)], self.biases['b{}'.format(i)]\n",
        "      prev_layer = self.prelu(tf.nn.conv2d(prev_layer, weights, strides=[1,1,1,1], padding='SAME') + biases, i)\n",
        "\n",
        "    # Expanding\n",
        "    expand_weights, expand_biases = self.weights['w{}'.format(m + 3)], self.biases['b{}'.format(m + 3)]\n",
        "    conv_expand = self.prelu(tf.nn.conv2d(prev_layer, expand_weights, strides=[1,1,1,1], padding='SAME') + expand_biases, 7)\n",
        "\n",
        "    # Deconvolution\n",
        "    deconv_output = [self.batch, self.label_size, self.label_size, self.c_dim]\n",
        "    deconv_stride = [1,  self.scale, self.scale, 1]\n",
        "    deconv_weights, deconv_biases = self.weights['w{}'.format(m + 4)], self.biases['b{}'.format(m + 4)]\n",
        "    conv_deconv = tf.nn.conv2d_transpose(conv_expand, deconv_weights, output_shape=deconv_output, strides=deconv_stride, padding='SAME') + deconv_biases\n",
        "\n",
        "    return conv_deconv\n",
        "\n",
        "  def prelu(self, _x, i):\n",
        "    \"\"\"\n",
        "    PreLU tensorflow implementation\n",
        "    \"\"\"\n",
        "    alphas = tf.get_variable('alpha{}'.format(i), _x.get_shape()[-1], initializer=tf.constant_initializer(0.0), dtype=tf.float32)\n",
        "    pos = tf.nn.relu(_x)\n",
        "    neg = alphas * (_x - abs(_x)) * 0.5\n",
        "\n",
        "    return pos + neg\n",
        "\n",
        "  def save(self, checkpoint_dir, step):\n",
        "    model_name = \"FSRCNN.model\"\n",
        "    model_dir = \"%s_%s\" % (\"fsrcnn\", self.label_size)\n",
        "    checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
        "\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "\n",
        "    self.saver.save(self.sess,\n",
        "                    os.path.join(checkpoint_dir, model_name),\n",
        "                    global_step=step)\n",
        "    print(os.path.join(checkpoint_dir, model_name))\n",
        "\n",
        "  def load(self, checkpoint_dir):\n",
        "    print(\" [*] Reading checkpoints...\")\n",
        "    model_dir = \"%s_%s\" % (\"fsrcnn\", self.label_size)\n",
        "    checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
        "\n",
        "    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
        "#     print(ckpt,ckpt.model_checkpoint_path)\n",
        "    if ckpt and ckpt.model_checkpoint_path:\n",
        "        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
        "        self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnPpwFJ83CX_",
        "colab_type": "code",
        "outputId": "4d03d5ae-d6c9-4264-b532-8f53a5499313",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        }
      },
      "source": [
        "from model import FSRCNN\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import pprint\n",
        "import os\n",
        "\n",
        "flags = tf.app.flags\n",
        "flags.DEFINE_boolean(\"fast\", False, \"Use the fast model (FSRCNN-s) [False]\")\n",
        "flags.DEFINE_integer(\"epoch\", 10, \"Number of epochs [10]\")\n",
        "flags.DEFINE_integer(\"batch_size\", 128, \"The size of batch images [128]\")\n",
        "flags.DEFINE_float(\"learning_rate\", 1e-3, \"The learning rate of gradient descent algorithm [1e-3]\")\n",
        "flags.DEFINE_float(\"momentum\", 0.9, \"The momentum value for the momentum SGD [0.9]\")\n",
        "flags.DEFINE_integer(\"c_dim\", 1, \"Dimension of image color [1]\")\n",
        "flags.DEFINE_integer(\"scale\", 3, \"The size of scale factor for preprocessing input image [3]\")\n",
        "flags.DEFINE_integer(\"stride\", 4, \"The size of stride to apply to input image [4]\")\n",
        "flags.DEFINE_string(\"checkpoint_dir\", \"checkpoint\", \"Name of checkpoint directory [checkpoint]\")\n",
        "flags.DEFINE_string(\"output_dir\", \"result\", \"Name of test output directory [result]\")\n",
        "flags.DEFINE_string(\"data_dir\", \"Train\", \"Name of data directory to train on [FastTrain]\")\n",
        "flags.DEFINE_boolean(\"train\", True, \"True for training, false for testing [True]\")\n",
        "flags.DEFINE_integer(\"threads\", 1, \"Number of processes to pre-process data with [1]\")\n",
        "flags.DEFINE_boolean(\"params\", False, \"Save weight and bias parameters [False]\")\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "pp = pprint.PrettyPrinter()\n",
        "\n",
        "def main(_):\n",
        "  pp.pprint(flags.FLAGS.__flags)\n",
        "\n",
        "  if FLAGS.fast:\n",
        "    FLAGS.checkpoint_dir = 'fast_{}'.format(FLAGS.checkpoint_dir)\n",
        "  if not os.path.exists(FLAGS.checkpoint_dir):\n",
        "    os.makedirs(FLAGS.checkpoint_dir)\n",
        "  if not os.path.exists(FLAGS.output_dir):\n",
        "    os.makedirs(FLAGS.output_dir)\n",
        "\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    fsrcnn = FSRCNN(sess, config=FLAGS)\n",
        "    fsrcnn.run()\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "  tf.app.run()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "from model import FSRCNN\n",
            "\n",
            "import numpy as np\n",
            "import tensorflow as tf\n",
            "\n",
            "import pprint\n",
            "import os\n",
            "\n",
            "flags = tf.app.flags\n",
            "flags.DEFINE_boolean(\"fast\", False, \"Use the fast model (FSRCNN-s) [False]\")\n",
            "flags.DEFINE_integer(\"epoch\", 10, \"Number of epochs [10]\")\n",
            "flags.DEFINE_integer(\"batch_size\", 128, \"The size of batch images [128]\")\n",
            "flags.DEFINE_float(\"learning_rate\", 1e-3, \"The learning rate of gradient descent algorithm [1e-3]\")\n",
            "flags.DEFINE_float(\"momentum\", 0.9, \"The momentum value for the momentum SGD [0.9]\")\n",
            "flags.DEFINE_integer(\"c_dim\", 1, \"Dimension of image color [1]\")\n",
            "flags.DEFINE_integer(\"scale\", 3, \"The size of scale factor for preprocessing input image [3]\")\n",
            "flags.DEFINE_integer(\"stride\", 4, \"The size of stride to apply to input image [4]\")\n",
            "flags.DEFINE_string(\"checkpoint_dir\", \"checkpoint\", \"Name of checkpoint directory [checkpoint]\")\n",
            "flags.DEFINE_string(\"output_dir\", \"result\", \"Name of test output directory [result]\")\n",
            "flags.DEFINE_string(\"data_dir\", \"Train\", \"Name of data directory to train on [FastTrain]\")\n",
            "flags.DEFINE_boolean(\"train\", True, \"True for training, false for testing [True]\")\n",
            "flags.DEFINE_integer(\"threads\", 1, \"Number of processes to pre-process data with [1]\")\n",
            "flags.DEFINE_boolean(\"params\", False, \"Save weight and bias parameters [False]\")\n",
            "\n",
            "FLAGS = flags.FLAGS\n",
            "\n",
            "pp = pprint.PrettyPrinter()\n",
            "\n",
            "def main(_):\n",
            "  pp.pprint(flags.FLAGS.__flags)\n",
            "\n",
            "  if FLAGS.fast:\n",
            "    FLAGS.checkpoint_dir = 'fast_{}'.format(FLAGS.checkpoint_dir)\n",
            "  if not os.path.exists(FLAGS.checkpoint_dir):\n",
            "    os.makedirs(FLAGS.checkpoint_dir)\n",
            "  if not os.path.exists(FLAGS.output_dir):\n",
            "    os.makedirs(FLAGS.output_dir)\n",
            "\n",
            "\n",
            "  with tf.Session() as sess:\n",
            "    fsrcnn = FSRCNN(sess, config=FLAGS)\n",
            "    fsrcnn.run()\n",
            "    \n",
            "if __name__ == '__main__':\n",
            "  tf.app.run()"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EMuo5JR3Luu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}